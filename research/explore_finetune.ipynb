{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismasadou/Documents/kuude/fulfulde-translator/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import jsonlines\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets Samples\n",
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n",
      "{'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.'}\n",
      "{'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}\n",
      "{'instruction': 'How can we reduce air pollution?', 'input': '', 'output': 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Response:\\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.'}\n",
      "{'instruction': 'Describe a time when you had to make a difficult decision.', 'input': '', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe a time when you had to make a difficult decision.\\n\\n### Response:\\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'}\n",
      "{'instruction': 'Identify the odd one out.', 'input': 'Twitter, Instagram, Telegram', 'output': 'Telegram', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nIdentify the odd one out.\\n\\n### Input:\\nTwitter, Instagram, Telegram\\n\\n### Response:\\nTelegram'}\n",
      "{'instruction': 'Explain why the following fraction is equivalent to 1/4', 'input': '4/16', 'output': 'The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain why the following fraction is equivalent to 1/4\\n\\n### Input:\\n4/16\\n\\n### Response:\\nThe fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.'}\n",
      "{'instruction': 'Write a short story in third person narration about a protagonist who has to make an important career decision.', 'input': '', 'output': 'John was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \\n\\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\\n\\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n### Response:\\nJohn was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \\n\\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\\n\\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him.'}\n",
      "{'instruction': 'Render a 3D model of a house', 'input': '', 'output': '<nooutput> This type of instruction cannot be fulfilled by a GPT model.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRender a 3D model of a house\\n\\n### Response:\\n<nooutput> This type of instruction cannot be fulfilled by a GPT model.'}\n",
      "{'instruction': 'Evaluate this sentence for spelling and grammar mistakes', 'input': 'He finnished his meal and left the resturant', 'output': 'He finished his meal and left the restaurant.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEvaluate this sentence for spelling and grammar mistakes\\n\\n### Input:\\nHe finnished his meal and left the resturant\\n\\n### Response:\\nHe finished his meal and left the restaurant.'}\n",
      "{'instruction': 'How did Julius Caesar die?', 'input': '', 'output': 'Julius Caesar was assassinated by a group of up to 60 conspirators, led by Gaius Cassius Longinus and Marcus Junius Brutus, in the Senate House on the Ides of March (15 March) of 44 BC.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow did Julius Caesar die?\\n\\n### Response:\\nJulius Caesar was assassinated by a group of up to 60 conspirators, led by Gaius Cassius Longinus and Marcus Junius Brutus, in the Senate House on the Ides of March (15 March) of 44 BC.'}\n",
      "{'instruction': 'What is the capital of France?', 'input': '', 'output': 'The capital of France is Paris.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the capital of France?\\n\\n### Response:\\nThe capital of France is Paris.'}\n",
      "{'instruction': 'Generate a list of ten items a person might need for a camping trip', 'input': '', 'output': '1. Tent\\n2. Sleeping bags\\n3. Flashlight\\n4. Matches/lighter\\n5. Insect repellent\\n6. Extra clothes\\n7. First aid kit\\n8. Camp stove\\n9. Cooking utensils\\n10. Biodegradable soaps/shampoos', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a list of ten items a person might need for a camping trip\\n\\n### Response:\\n1. Tent\\n2. Sleeping bags\\n3. Flashlight\\n4. Matches/lighter\\n5. Insect repellent\\n6. Extra clothes\\n7. First aid kit\\n8. Camp stove\\n9. Cooking utensils\\n10. Biodegradable soaps/shampoos'}\n",
      "{'instruction': 'Discuss the causes of the Great Depression', 'input': '', 'output': 'The Great Depression was a severe global economic downturn that lasted from 1929 to 1939. It was caused by several factors, including the stock market crash of 1929, structural weaknesses in the economy, low consumer spending, high unemployment, and a lack of confidence in the banking system. Other contributing factors included overproduction of commodities, reliance on borrowing to finance investments, agricultural overproduction, and a decrease in international trade.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDiscuss the causes of the Great Depression\\n\\n### Response:\\nThe Great Depression was a severe global economic downturn that lasted from 1929 to 1939. It was caused by several factors, including the stock market crash of 1929, structural weaknesses in the economy, low consumer spending, high unemployment, and a lack of confidence in the banking system. Other contributing factors included overproduction of commodities, reliance on borrowing to finance investments, agricultural overproduction, and a decrease in international trade.'}\n",
      "{'instruction': 'Classify the following into animals, plants, and minerals', 'input': 'Oak tree, copper ore, elephant', 'output': 'Oak tree: Plant\\n Copper ore: Mineral\\n Elephant: Animal', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nClassify the following into animals, plants, and minerals\\n\\n### Input:\\nOak tree, copper ore, elephant\\n\\n### Response:\\nOak tree: Plant\\n Copper ore: Mineral\\n Elephant: Animal'}\n",
      "{'instruction': 'Explain the use of word embeddings in Natural Language Processing', 'input': '', 'output': 'Word embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured. Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies. They allow for words to be processed as numerical values, giving machines an easier way to perform NLP tasks.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain the use of word embeddings in Natural Language Processing\\n\\n### Response:\\nWord embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured. Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies. They allow for words to be processed as numerical values, giving machines an easier way to perform NLP tasks.'}\n",
      "{'instruction': 'Describe the function of a computer motherboard', 'input': '', 'output': 'A computer motherboard is the main circuit board in a computer. It is the backbone of the system, connecting all other components such as the processor, RAM, storage, graphics card and peripheral ports. It provides communication pathways between all of these components, allowing them to interact and work together. The motherboard is responsible for regulating power, and also houses slots for expansion cards, as well as ports for USB, audio, Ethernet, and other connection types.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the function of a computer motherboard\\n\\n### Response:\\nA computer motherboard is the main circuit board in a computer. It is the backbone of the system, connecting all other components such as the processor, RAM, storage, graphics card and peripheral ports. It provides communication pathways between all of these components, allowing them to interact and work together. The motherboard is responsible for regulating power, and also houses slots for expansion cards, as well as ports for USB, audio, Ethernet, and other connection types.'}\n",
      "{'instruction': 'Reverse engineer this code to create a new version', 'input': 'def factorialize(num):\\n  factorial = 1\\n  for i in range(1, num):\\n    factorial *= i\\n  \\n  return factorial', 'output': 'def factorialize(num):\\n  factorial = 1\\n  for i in range(num, 0, -1):\\n    factorial *= i\\n  \\n  return factorial', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nReverse engineer this code to create a new version\\n\\n### Input:\\ndef factorialize(num):\\n  factorial = 1\\n  for i in range(1, num):\\n    factorial *= i\\n  \\n  return factorial\\n\\n### Response:\\ndef factorialize(num):\\n  factorial = 1\\n  for i in range(num, 0, -1):\\n    factorial *= i\\n  \\n  return factorial'}\n",
      "{'instruction': 'Propose an ethical solution to the problem of data privacy', 'input': '', 'output': 'One ethical solution to the problem of data privacy is to implement an opt-in data collection policy. This means that companies cannot collect and use personal data without the explicit permission of the data owners. This approach gives users the power to determine which data companies can use and for what purpose. Companies should also be transparent about how they use the collected data, so that users can make informed decisions and have control over their own data.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPropose an ethical solution to the problem of data privacy\\n\\n### Response:\\nOne ethical solution to the problem of data privacy is to implement an opt-in data collection policy. This means that companies cannot collect and use personal data without the explicit permission of the data owners. This approach gives users the power to determine which data companies can use and for what purpose. Companies should also be transparent about how they use the collected data, so that users can make informed decisions and have control over their own data.'}\n",
      "{'instruction': 'Generate three verbs that mean the same as \"to apologize\"', 'input': '', 'output': 'Acknowledge, express regret, make amends.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate three verbs that mean the same as \"to apologize\"\\n\\n### Response:\\nAcknowledge, express regret, make amends.'}\n"
     ]
    }
   ],
   "source": [
    "num_samples = 20\n",
    "print(\"Datasets Samples\")\n",
    "top_m = list(itertools.islice(instruction_tuned_dataset, num_samples))\n",
    "for sample in top_m:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_with_input = \"\"\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: \n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_without_input = \"\"\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for sample in top_m:\n",
    "    if sample[\"input\"]:\n",
    "        prompt = prompt_template_with_input.format(instruction=sample[\"instruction\"], input=sample[\"input\"])\n",
    "    else:\n",
    "        prompt = prompt_template_without_input.format(instruction=sample[\"instruction\"])\n",
    "\n",
    "    processed_data.append({\"prompt\": prompt, \"output\": sample[\"output\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': 'He finished his meal and left the restaurant.',\n",
      " 'prompt': '\"\"Below is an instruction that describes a task, paired with an '\n",
      "           'input that provides further context. Write a response that '\n",
      "           'appropriately completes the request.\\n'\n",
      "           '\\n'\n",
      "           '### Instruction: \\n'\n",
      "           'Evaluate this sentence for spelling and grammar mistakes\\n'\n",
      "           '\\n'\n",
      "           '### Input:\\n'\n",
      "           'He finnished his meal and left the resturant\\n'\n",
      "           '\\n'\n",
      "           '### Response:'}\n"
     ]
    }
   ],
   "source": [
    "pprint(processed_data[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"alpaca_instruction_tuned.jsonl\", \"w\") as writer:\n",
    "    writer.write_all(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "### lamini/alpaca\n",
    "dataset_path_hf = \"lamini/alpaca\"\n",
    "dataset_hf = load_dataset(dataset_path_hf)\n",
    "print(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Below is an instruction that describes a task. Write a response '\n",
      "          'that appropriately completes the request.\\n'\n",
      "          '\\n'\n",
      "          '### Instruction:\\n'\n",
      "          'Give three tips for staying healthy.\\n'\n",
      "          '\\n'\n",
      "          '### Response:',\n",
      " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits '\n",
      "           'and vegetables. \\n'\n",
      "           '2. Exercise regularly to keep your body active and strong. \\n'\n",
      "           '3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "top_n = itertools.islice(dataset_hf[\"train\"], num_samples)\n",
    "pprint(top_n.__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "input = \"Tell me how to teach my cat to high five.\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_tuned_model = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\")\n",
    "# non_tuned_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_length=1024, max_output_length=1000):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs.to(device), max_length=max_output_length)\n",
    "    response_with_prompt = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response_with_prompt[0][len(text):]\n",
    "    return response_with_prompt, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = finetuning_dataset[\"test\"][0]\n",
    "pprint(test_sample)\n",
    "\n",
    "a,b = inference(test_sample[\"question\"], model, tokenizer)\n",
    "print('\\n\\n')\n",
    "\n",
    "pprint(a)\n",
    "print('\\n\\n')\n",
    "pprint(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "instruction_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = inference(test_sample[\"question\"], instruction_model, tokenizer)\n",
    "print('\\n\\n')\n",
    "pprint(a)\n",
    "print('\\n\\n')\n",
    "pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12764, 13, 849, 403, 1175, 32], [4374, 13, 6296, 15], [42, 717, 1175]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = ['Hi, how are good?','Yes, indeed.', 'I am good']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12764, 13, 849, 403, 1175, 32], [4374, 13, 6296, 15, 0, 0], [42, 717, 1175, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_pad_longest = tokenizer(list_texts, padding=True)\n",
    "print(encoded_pad_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12764, 13, 849, 403, 1175], [4374, 13, 6296, 15], [42, 717, 1175]]\n"
     ]
    }
   ],
   "source": [
    "encoded_truncation = tokenizer(list_texts, max_length=5, truncation=True)\n",
    "print(encoded_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(list_texts, padding=True, max_length=10, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/xlg3nq8x63j3nxrs9x50yw9m0000gn/T/ipykernel_36411/295234309.py:4: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  instruction_dataset_df = pd.read_json(filename, lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlamini_docs.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m instruction_dataset_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m examples \u001b[38;5;241m=\u001b[39m instruction_dataset_df\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m examples \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m examples:\n",
      "File \u001b[0;32m~/Documents/kuude/fulfulde-translator/.venv/lib/python3.13/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/kuude/fulfulde-translator/.venv/lib/python3.13/site-packages/pandas/io/json/_json.py:1023\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1022\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Documents/kuude/fulfulde-translator/.venv/lib/python3.13/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/kuude/fulfulde-translator/.venv/lib/python3.13/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/kuude/fulfulde-translator/.venv/lib/python3.13/site-packages/pandas/io/json/_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m     )\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1409\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# filename = \"lamini_docs.jsonl\"\n",
    "# instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "# examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "# if \"question\" in examples and \"answer\" in examples:\n",
    "#   text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "# elif \"instruction\" in examples and \"response\" in examples:\n",
    "#   text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "# elif \"input\" in examples and \"output\" in examples:\n",
    "#   text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "# else:\n",
    "#   text = examples[\"text\"][0]\n",
    "\n",
    "# prompt_template = \"\"\"### Question:\n",
    "# {question}\n",
    "\n",
    "# ### Answer:\"\"\"\n",
    "\n",
    "# num_examples = len(examples[\"question\"])\n",
    "# finetuning_dataset = []\n",
    "# for i in range(num_examples):\n",
    "#   question = examples[\"question\"][i]\n",
    "#   answer = examples[\"answer\"][i]\n",
    "#   text_with_prompt_template = prompt_template.format(question=question)\n",
    "#   finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "# from pprint import pprint\n",
    "# print(\"One datapoint in the finetuning dataset:\")\n",
    "# pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Yes, the code includes methods for submitting jobs, checking job '\n",
      "           'status, and retrieving job results. It also includes a method for '\n",
      "           'canceling jobs. Additionally, there is a method for sampling '\n",
      "           'multiple outputs from a model, which could be useful for '\n",
      "           'long-running tasks.',\n",
      " 'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'input_ids': [5804,\n",
      "               309,\n",
      "               1089,\n",
      "               1491,\n",
      "               670,\n",
      "               253,\n",
      "               2127,\n",
      "               434,\n",
      "               2746,\n",
      "               281,\n",
      "               10885,\n",
      "               1048,\n",
      "               14,\n",
      "               24220,\n",
      "               8892,\n",
      "               285,\n",
      "               4114,\n",
      "               7375,\n",
      "               32,\n",
      "               4374,\n",
      "               13,\n",
      "               253,\n",
      "               2127,\n",
      "               3797,\n",
      "               3082,\n",
      "               323,\n",
      "               29315,\n",
      "               7375,\n",
      "               13,\n",
      "               12669,\n",
      "               2628,\n",
      "               3708,\n",
      "               13,\n",
      "               285,\n",
      "               48484,\n",
      "               2628,\n",
      "               1543,\n",
      "               15,\n",
      "               733,\n",
      "               671,\n",
      "               3797,\n",
      "               247,\n",
      "               1332,\n",
      "               323,\n",
      "               14002,\n",
      "               272,\n",
      "               7375,\n",
      "               15,\n",
      "               9157,\n",
      "               13,\n",
      "               627,\n",
      "               310,\n",
      "               247,\n",
      "               1332,\n",
      "               323,\n",
      "               10491,\n",
      "               2709,\n",
      "               18012,\n",
      "               432,\n",
      "               247,\n",
      "               1566,\n",
      "               13,\n",
      "               534,\n",
      "               812,\n",
      "               320,\n",
      "               4217,\n",
      "               323,\n",
      "               1048,\n",
      "               14,\n",
      "               24220,\n",
      "               8892,\n",
      "               15],\n",
      " 'labels': [5804,\n",
      "            309,\n",
      "            1089,\n",
      "            1491,\n",
      "            670,\n",
      "            253,\n",
      "            2127,\n",
      "            434,\n",
      "            2746,\n",
      "            281,\n",
      "            10885,\n",
      "            1048,\n",
      "            14,\n",
      "            24220,\n",
      "            8892,\n",
      "            285,\n",
      "            4114,\n",
      "            7375,\n",
      "            32,\n",
      "            4374,\n",
      "            13,\n",
      "            253,\n",
      "            2127,\n",
      "            3797,\n",
      "            3082,\n",
      "            323,\n",
      "            29315,\n",
      "            7375,\n",
      "            13,\n",
      "            12669,\n",
      "            2628,\n",
      "            3708,\n",
      "            13,\n",
      "            285,\n",
      "            48484,\n",
      "            2628,\n",
      "            1543,\n",
      "            15,\n",
      "            733,\n",
      "            671,\n",
      "            3797,\n",
      "            247,\n",
      "            1332,\n",
      "            323,\n",
      "            14002,\n",
      "            272,\n",
      "            7375,\n",
      "            15,\n",
      "            9157,\n",
      "            13,\n",
      "            627,\n",
      "            310,\n",
      "            247,\n",
      "            1332,\n",
      "            323,\n",
      "            10491,\n",
      "            2709,\n",
      "            18012,\n",
      "            432,\n",
      "            247,\n",
      "            1566,\n",
      "            13,\n",
      "            534,\n",
      "            812,\n",
      "            320,\n",
      "            4217,\n",
      "            323,\n",
      "            1048,\n",
      "            14,\n",
      "            24220,\n",
      "            8892,\n",
      "            15],\n",
      " 'question': \"Can I find information about the code's approach to handling \"\n",
      "             'long-running tasks and background jobs?'}\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name, split=\"train\")\n",
    "pprint(finetuning_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I evaluate the performance and quality of the generated text from Lamini models?There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n",
      "[[ 2347   476   309  7472   253  3045   285  3290   273   253  4561  2505\n",
      "    432   418  4988    74  3210    32  2512   403  2067 17082   326   476\n",
      "    320   908   281  7472   253  3045   285  3290   273  4561  2505   432\n",
      "    418  4988    74  3210    13  1690 44229   414    13   378  1843    54\n",
      "   4868    13   285  1966  7103    15  3545 12813   414  5593   849   973\n",
      "    253  1566 26295   253  1735  3159   275   247  3425    13  1223   378\n",
      "   1843    54  4868  5593   253 14259   875   253  4561  2505   285   247\n",
      "   3806  2505    15  8801  7103  8687  1907  1966 16006  2281   253  3290\n",
      "    273   253  4561  2505  1754   327  2616   824   347 25253    13  2938\n",
      "   1371    13   285 17200    15   733   310  8521   281   897   247  5019\n",
      "    273   841 17082   323   247 11088  7103   273   253  1566   434  3045\n",
      "     15]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "print(text)\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2347   476   309  7472   253  3045   285  3290   273   253  4561  2505\n",
      "    432   418  4988    74  3210    32  2512   403  2067 17082   326   476\n",
      "    320   908   281  7472   253  3045   285  3290   273  4561  2505   432\n",
      "    418  4988    74  3210    13  1690 44229   414    13   378  1843    54\n",
      "   4868    13   285  1966  7103    15  3545 12813   414  5593   849   973\n",
      "    253  1566 26295   253  1735  3159   275   247  3425    13  1223   378\n",
      "   1843    54  4868  5593   253 14259   875   253  4561  2505   285   247\n",
      "   3806  2505    15  8801  7103  8687  1907  1966 16006  2281   253  3290\n",
      "    273   253  4561  2505  1754   327  2616   824   347 25253    13  2938\n",
      "   1371    13   285 17200    15   733   310  8521   281   897   247  5019\n",
      "    273   841 17082   323   247 11088  7103   273   253  1566   434  3045\n",
      "     15]]\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = finetuning_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True,\n",
    ")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1134\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 126\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
