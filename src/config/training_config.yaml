data:
  max_length: 128
  seed: 42
  train_test_split: 0.1
evaluation:
  eval_steps: 1000
  metric: sacrebleu
  save_steps: 1000
models:
  m2m100:
    language_codes:
      english: en_Latn
      french: fr_Latn
      fulfulde: ff_Latn
    model_name: facebook/m2m100_418M
  nllb:
    language_codes:
      english: en
      french: fr
      fulfulde: ff
    model_name: facebook/nllb-200-distilled-600M
peft:
  bias: none
  lora_alpha: 16
  lora_dropout: 0.1
  r: 8
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
training:
  bf16: false
  fp16: false
  gradient_accumulation_steps: 2
  learning_rate: 0.0002
  max_grad_norm: 1.0
  num_training_epochs: 15
  num_warmup_steps: 200
  optimizer: adamw_torch
  per_device_eval_batch_size: 4
  per_device_train_batch_size: 4
  scheduler: cosine
  weight_decay: 0.01
